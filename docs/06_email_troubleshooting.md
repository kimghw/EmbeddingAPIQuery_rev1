# ğŸš¨ ì´ë©”ì¼ ì‹œìŠ¤í…œ ë¬¸ì œ í•´ê²° ê°€ì´ë“œ

## ğŸ“‹ ê°œìš”

ì´ë©”ì¼ ì‹œìŠ¤í…œ ìš´ì˜ ì¤‘ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë¬¸ì œë“¤ê³¼ í•´ê²° ë°©ë²•ì„ ì •ë¦¬í•œ ë¬¸ì„œì…ë‹ˆë‹¤.

## ğŸ” ì£¼ìš” ë¬¸ì œ ë° í•´ê²°ì±…

### 1. Qdrant ë¬´í•œ ë£¨í”„ ë¬¸ì œ âš ï¸

#### ë¬¸ì œ ì¦ìƒ
```
- /api/emails/list API í˜¸ì¶œ ì‹œ ë¬´í•œ ëŒ€ê¸° (hang)
- ì„œë²„ CPU ì‚¬ìš©ë¥  ê¸‰ì¦
- ì‘ë‹µ ì—†ìŒ (timeout)
```

#### ê·¼ë³¸ ì›ì¸
```python
# âŒ ì˜ëª»ëœ Qdrant scroll API ì‚¬ìš©
async def count_embeddings(self, collection_name: str) -> int:
    while True:  # ë¬´í•œ ë£¨í”„ ìœ„í—˜!
        result = self.client.scroll(
            collection_name=collection_name,
            offset=offset,  # âŒ ë‹¨ìˆœ ìˆ«ì ì¦ê°€ ë°©ì‹
        )
        offset += batch_size  # âŒ Qdrantì—ì„œ ì‘ë™í•˜ì§€ ì•ŠìŒ
```

#### í•´ê²° ë°©ë²•
```python
# âœ… ì˜¬ë°”ë¥¸ Qdrant scroll API ì‚¬ìš©
async def count_embeddings(self, collection_name: str) -> int:
    count = 0
    next_page_offset = None
    max_iterations = 1000  # ì•ˆì „ì¥ì¹˜
    
    while iterations < max_iterations:
        result = self.client.scroll(
            collection_name=collection_name,
            offset=next_page_offset,  # âœ… next_page_offset ì‚¬ìš©
        )
        
        points, next_page_offset = result[0], result[1]
        count += len(points)
        
        if len(points) == 0 or next_page_offset is None:
            break  # âœ… ëª…í™•í•œ ì¢…ë£Œ ì¡°ê±´
    
    return count
```

#### ì˜ˆë°© ì¡°ì¹˜
- Qdrant ê³µì‹ ë¬¸ì„œ ì¤€ìˆ˜
- ë¬´í•œ ë£¨í”„ ë°©ì§€ë¥¼ ìœ„í•œ max_iterations ì„¤ì •
- í˜ì´ì§€ë„¤ì´ì…˜ ë¡œì§ í…ŒìŠ¤íŠ¸ ê°•í™”

### 2. ì´ë©”ì¼ JSON íŒŒì‹± ì˜¤ë¥˜

#### ë¬¸ì œ ì¦ìƒ
```
ValueError: Invalid email JSON structure
KeyError: 'value'
AttributeError: 'NoneType' object has no attribute 'get'
```

#### ì¼ë°˜ì ì¸ ì›ì¸
1. **Microsoft Graph JSON êµ¬ì¡° ë¶ˆì¼ì¹˜**
2. **í•„ìˆ˜ í•„ë“œ ëˆ„ë½**
3. **ì˜ëª»ëœ ë°ì´í„° íƒ€ì…**

#### í•´ê²° ë°©ë²•

##### 1. JSON êµ¬ì¡° ê²€ì¦ ê°•í™”
```python
def validate_json_structure(self, json_data: Dict[str, Any]) -> bool:
    # Microsoft Graph êµ¬ì¡° í™•ì¸
    if "@odata.context" not in json_data:
        logger.error("Missing @odata.context in JSON")
        return False
    
    # value ë°°ì—´ í™•ì¸
    if "value" not in json_data:
        logger.error("Missing 'value' array in JSON")
        return False
    
    # ì²« ë²ˆì§¸ ì´ë©”ì¼ êµ¬ì¡° í™•ì¸
    if json_data["value"]:
        first_email = json_data["value"][0]
        required_fields = ["id", "subject", "body", "sender"]
        
        for field in required_fields:
            if field not in first_email:
                logger.error(f"Missing required field: {field}")
                return False
    
    return True
```

##### 2. ì•ˆì „í•œ ë°ì´í„° ì¶”ì¶œ
```python
def safe_extract_email_data(self, email_data: Dict[str, Any]) -> Dict[str, Any]:
    return {
        "id": email_data.get("id", ""),
        "subject": email_data.get("subject", "No Subject"),
        "body_content": email_data.get("body", {}).get("content", ""),
        "sender_name": email_data.get("sender", {}).get("emailAddress", {}).get("name", "Unknown"),
        "sender_address": email_data.get("sender", {}).get("emailAddress", {}).get("address", ""),
        "created_datetime": self._parse_datetime(email_data.get("createdDateTime")),
    }
```

##### 3. ì—ëŸ¬ ë³µêµ¬ ë¡œì§
```python
async def load_from_json_with_recovery(self, json_data: Dict[str, Any]) -> List[Email]:
    emails = []
    failed_count = 0
    
    for email_data in json_data.get("value", []):
        try:
            email = Email.from_graph_api(email_data)
            emails.append(email)
        except Exception as e:
            failed_count += 1
            logger.warning(f"Failed to parse email {email_data.get('id', 'unknown')}: {e}")
            continue
    
    logger.info(f"Successfully parsed {len(emails)} emails, failed: {failed_count}")
    return emails
```

### 3. ì„ë² ë”© ìƒì„± ì‹¤íŒ¨

#### ë¬¸ì œ ì¦ìƒ
```
OpenAI API Error: Rate limit exceeded
OpenAI API Error: Invalid API key
TimeoutError: Request timeout
```

#### í•´ê²° ë°©ë²•

##### 1. Rate Limiting ì²˜ë¦¬
```python
import asyncio
from tenacity import retry, stop_after_attempt, wait_exponential

class OpenAIEmbeddingAdapter:
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def embed_texts(self, texts: List[str]) -> List[List[float]]:
        try:
            # ë°°ì¹˜ í¬ê¸° ì œí•œ
            batch_size = 100
            all_embeddings = []
            
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                
                response = await self.client.embeddings.create(
                    model=self.model_name,
                    input=batch
                )
                
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
                
                # Rate limiting ë°©ì§€
                await asyncio.sleep(0.1)
            
            return all_embeddings
            
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise
```

##### 2. ëŒ€ì²´ ì„ë² ë”© ëª¨ë¸
```python
async def embed_texts_with_fallback(self, texts: List[str]) -> List[List[float]]:
    try:
        # 1ì°¨: OpenAI API ì‹œë„
        return await self.openai_embedding.embed_texts(texts)
    except Exception as e:
        logger.warning(f"OpenAI embedding failed: {e}, trying fallback")
        
        try:
            # 2ì°¨: ë¡œì»¬ ëª¨ë¸ ì‹œë„
            return await self.local_embedding.embed_texts(texts)
        except Exception as e2:
            logger.error(f"All embedding methods failed: {e2}")
            # 3ì°¨: ë”ë¯¸ ë²¡í„° ë°˜í™˜ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
            return [[0.0] * 1536 for _ in texts]
```

##### 3. ì»¨í…ì¸  ì „ì²˜ë¦¬
```python
def preprocess_content_for_embedding(self, content: str) -> str:
    # HTML íƒœê·¸ ì œê±°
    import re
    clean_content = re.sub(r'<[^>]+>', '', content)
    
    # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
    clean_content = re.sub(r'[^\w\sê°€-í£]', ' ', clean_content)
    
    # ê³µë°± ì •ë¦¬
    clean_content = re.sub(r'\s+', ' ', clean_content).strip()
    
    # ê¸¸ì´ ì œí•œ (OpenAI í† í° ì œí•œ)
    max_length = 8000  # ì•ˆì „ ë§ˆì§„
    if len(clean_content) > max_length:
        clean_content = clean_content[:max_length]
    
    return clean_content
```

### 4. ë²¡í„° ìŠ¤í† ì–´ ì—°ê²° ë¬¸ì œ

#### ë¬¸ì œ ì¦ìƒ
```
ConnectionError: Could not connect to Qdrant
TimeoutError: Qdrant request timeout
QdrantException: Collection not found
```

#### í•´ê²° ë°©ë²•

##### 1. ì—°ê²° ìƒíƒœ í™•ì¸
```python
async def check_vector_store_health(self) -> bool:
    try:
        # Qdrant í—¬ìŠ¤ ì²´í¬
        health = await self.vector_store.health_check()
        if not health:
            logger.error("Qdrant health check failed")
            return False
        
        # ì»¬ë ‰ì…˜ ì¡´ì¬ í™•ì¸
        if not await self.vector_store.collection_exists("emails"):
            logger.warning("Email collection does not exist, creating...")
            await self.vector_store.create_collection(
                collection_name="emails",
                dimension=1536
            )
        
        return True
        
    except Exception as e:
        logger.error(f"Vector store health check failed: {e}")
        return False
```

##### 2. ìë™ ë³µêµ¬ ë¡œì§
```python
async def ensure_vector_store_ready(self) -> VectorStorePort:
    # 1ì°¨: Qdrant ì—°ê²° ì‹œë„
    try:
        if await self.qdrant_store.health_check():
            return self.qdrant_store
    except Exception as e:
        logger.warning(f"Qdrant connection failed: {e}")
    
    # 2ì°¨: FAISS ë¡œì»¬ ìŠ¤í† ì–´ ì‹œë„
    try:
        await self.faiss_store.initialize()
        logger.info("Switched to FAISS vector store")
        return self.faiss_store
    except Exception as e:
        logger.warning(f"FAISS initialization failed: {e}")
    
    # 3ì°¨: Mock ìŠ¤í† ì–´ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)
    logger.warning("Using Mock vector store as fallback")
    return self.mock_store
```

##### 3. ì¬ì—°ê²° ë©”ì»¤ë‹ˆì¦˜
```python
class ResilientVectorStore:
    def __init__(self, primary_store, fallback_store):
        self.primary_store = primary_store
        self.fallback_store = fallback_store
        self.current_store = primary_store
        self.last_health_check = 0
        self.health_check_interval = 60  # 60ì´ˆ
    
    async def add_embeddings(self, embeddings: List[Embedding], collection_name: str):
        if await self._should_check_health():
            await self._check_and_switch_if_needed()
        
        try:
            return await self.current_store.add_embeddings(embeddings, collection_name)
        except Exception as e:
            logger.error(f"Primary store failed: {e}, switching to fallback")
            self.current_store = self.fallback_store
            return await self.current_store.add_embeddings(embeddings, collection_name)
```

### 5. ë©”ëª¨ë¦¬ ë¶€ì¡± ë¬¸ì œ

#### ë¬¸ì œ ì¦ìƒ
```
MemoryError: Unable to allocate memory
OutOfMemoryError: CUDA out of memory
Process killed (OOM)
```

#### í•´ê²° ë°©ë²•

##### 1. ë°°ì¹˜ ì²˜ë¦¬ ìµœì í™”
```python
async def process_large_email_batch(self, emails: List[Email]) -> Dict[str, Any]:
    batch_size = 50  # ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
    total_processed = 0
    
    for i in range(0, len(emails), batch_size):
        batch = emails[i:i + batch_size]
        
        try:
            # ë°°ì¹˜ ì²˜ë¦¬
            result = await self._process_email_batch(batch)
            total_processed += result['processed_count']
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            import gc
            gc.collect()
            
            # ì§„í–‰ ìƒí™© ë¡œê¹…
            logger.info(f"Processed {total_processed}/{len(emails)} emails")
            
        except Exception as e:
            logger.error(f"Batch processing failed: {e}")
            continue
    
    return {"total_processed": total_processed}
```

##### 2. ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
```python
async def process_large_json_file(self, file_path: str):
    import ijson
    
    with open(file_path, 'rb') as file:
        # JSON ìŠ¤íŠ¸ë¦¬ë° íŒŒì‹±
        emails = ijson.items(file, 'value.item')
        
        batch = []
        batch_size = 100
        
        for email_data in emails:
            batch.append(email_data)
            
            if len(batch) >= batch_size:
                # ë°°ì¹˜ ì²˜ë¦¬
                await self._process_email_batch(batch)
                batch = []
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                gc.collect()
        
        # ë§ˆì§€ë§‰ ë°°ì¹˜ ì²˜ë¦¬
        if batch:
            await self._process_email_batch(batch)
```

##### 3. ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```python
import psutil
import os

def monitor_memory_usage():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    
    memory_mb = memory_info.rss / 1024 / 1024
    logger.info(f"Current memory usage: {memory_mb:.2f} MB")
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ê²½ê³ 
    if memory_mb > 1000:  # 1GB
        logger.warning(f"High memory usage detected: {memory_mb:.2f} MB")
        
        # ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜
        import gc
        gc.collect()
```

### 6. API ì‘ë‹µ ì§€ì—° ë¬¸ì œ

#### ë¬¸ì œ ì¦ìƒ
```
Request timeout
Slow API response (> 30s)
Client disconnection
```

#### í•´ê²° ë°©ë²•

##### 1. ë¹„ë™ê¸° ì²˜ë¦¬
```python
from fastapi import BackgroundTasks

@router.post("/emails/process")
async def process_emails_async(
    email_data: dict,
    background_tasks: BackgroundTasks
):
    # ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜
    task_id = str(uuid.uuid4())
    
    # ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬
    background_tasks.add_task(
        process_emails_background,
        task_id,
        email_data
    )
    
    return {
        "task_id": task_id,
        "status": "processing",
        "message": "Email processing started"
    }

async def process_emails_background(task_id: str, email_data: dict):
    try:
        result = await email_usecase.process_emails_from_json(email_data)
        # ê²°ê³¼ë¥¼ ìºì‹œë‚˜ DBì— ì €ì¥
        await save_task_result(task_id, result)
    except Exception as e:
        await save_task_error(task_id, str(e))
```

##### 2. ì§„í–‰ ìƒí™© ì¶”ì 
```python
@router.get("/emails/process/{task_id}/status")
async def get_processing_status(task_id: str):
    status = await get_task_status(task_id)
    
    return {
        "task_id": task_id,
        "status": status.get("status", "unknown"),
        "progress": status.get("progress", 0),
        "result": status.get("result"),
        "error": status.get("error")
    }
```

##### 3. íƒ€ì„ì•„ì›ƒ ì„¤ì •
```python
from fastapi import HTTPException
import asyncio

async def process_with_timeout(email_data: dict, timeout: int = 300):
    try:
        result = await asyncio.wait_for(
            email_usecase.process_emails_from_json(email_data),
            timeout=timeout
        )
        return result
    except asyncio.TimeoutError:
        raise HTTPException(
            status_code=408,
            detail=f"Processing timeout after {timeout} seconds"
        )
```

## ğŸ”§ ë””ë²„ê¹… ë„êµ¬

### 1. ë¡œê·¸ ë¶„ì„
```bash
# ì´ë©”ì¼ ì²˜ë¦¬ ê´€ë ¨ ë¡œê·¸ í•„í„°ë§
grep "email" logs/app.log | tail -100

# ì—ëŸ¬ ë¡œê·¸ë§Œ í™•ì¸
grep "ERROR" logs/app.log | grep "email"

# íŠ¹ì • ì‹œê°„ëŒ€ ë¡œê·¸ í™•ì¸
grep "2025-05-29 20:" logs/app.log
```

### 2. í—¬ìŠ¤ ì²´í¬ ìŠ¤í¬ë¦½íŠ¸
```python
# check_email_system_health.py
async def check_email_system_health():
    checks = {
        "qdrant_connection": False,
        "openai_api": False,
        "email_collection": False,
        "embedding_generation": False
    }
    
    try:
        # Qdrant ì—°ê²° í™•ì¸
        health = await vector_store.health_check()
        checks["qdrant_connection"] = health
        
        # OpenAI API í™•ì¸
        test_embedding = await embedding_model.embed_texts(["test"])
        checks["openai_api"] = len(test_embedding) > 0
        
        # ì´ë©”ì¼ ì»¬ë ‰ì…˜ í™•ì¸
        exists = await vector_store.collection_exists("emails")
        checks["email_collection"] = exists
        
        # ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
        test_result = await embedding_model.embed_texts(["Hello world"])
        checks["embedding_generation"] = len(test_result[0]) == 1536
        
    except Exception as e:
        logger.error(f"Health check failed: {e}")
    
    return checks
```

### 3. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
```python
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        
        try:
            result = await func(*args, **kwargs)
            execution_time = time.time() - start_time
            
            logger.info(f"{func.__name__} completed in {execution_time:.2f}s")
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"{func.__name__} failed after {execution_time:.2f}s: {e}")
            raise
    
    return wrapper
```

## ğŸ“Š ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### 1. ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
# ì£¼ìš” ë©”íŠ¸ë¦­
metrics = {
    "emails_processed_total": 0,
    "emails_failed_total": 0,
    "embedding_generation_time_avg": 0.0,
    "vector_store_operations_total": 0,
    "api_response_time_avg": 0.0,
    "memory_usage_mb": 0.0,
    "error_rate_percent": 0.0
}
```

### 2. ì•Œë¦¼ ì„¤ì •
```python
async def send_alert_if_needed(metric_name: str, value: float, threshold: float):
    if value > threshold:
        alert_message = f"ALERT: {metric_name} = {value} (threshold: {threshold})"
        
        # ë¡œê·¸ ê¸°ë¡
        logger.critical(alert_message)
        
        # ì™¸ë¶€ ì•Œë¦¼ (Slack, ì´ë©”ì¼ ë“±)
        await send_external_alert(alert_message)
```

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [ì´ë©”ì¼ ì‹œìŠ¤í…œ ê°€ì´ë“œ](05_email_system_guide.md)
- [ì„±ëŠ¥ ìµœì í™”](07_performance_optimization.md)
- [ì¼ë°˜ì ì¸ ë¬¸ì œ](08_common_issues.md)
- [Qdrant ë¬´í•œ ë£¨í”„ í•´ê²° ë³´ê³ ì„œ](qdrant_infinite_loop_fix_report.md)

---

**ì‘ì„±ì¼**: 2025-05-29  
**ë²„ì „**: 1.0  
**ìƒíƒœ**: í™œì„±
